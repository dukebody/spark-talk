{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:9461d4e40049fbb8427e840b6ed149f2e6723e4aa064a63e37b274150088a0fd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MovieLens 10M Dataset Analysis and Collaborative Filtering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to use Spark to perform some basic analysis of the MovieLens dataset with 10 million movie ratings and later generate recommendations using the Alternating Least Squares algorithm.\n",
      "\n",
      "For the dataset analysis we will showcase first RDDs and later the newer DataFrame structure, introduced in Spark 1.3."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data comes in two files: `ratings.dat`, where each line has the structure `userid::movieid::rating::timestamp` and `movies.dat`, where each line is of the form `movieid::name::genres`.\n",
      "\n",
      "First, we read the beggining of the two files to check the structure:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "baseDir = os.path.join('home', 'dukebody', 'Downloads', 'ml-10M100K')\n",
      "baseDir = os.path.join('ml-20m')\n",
      "\n",
      "ratingsFilename = os.path.join(baseDir, 'ratings.csv')  # ~ 500MB\n",
      "moviesFilename = os.path.join(baseDir, 'movies.csv')  # ~ 1.4 MB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head $ratingsFilename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "userId,movieId,rating,timestamp\r",
        "\r\n",
        "1,2,3.5,1112486027\r",
        "\r\n",
        "1,29,3.5,1112484676\r",
        "\r\n",
        "1,32,3.5,1112484819\r",
        "\r\n",
        "1,47,3.5,1112484727\r",
        "\r\n",
        "1,50,3.5,1112484580\r",
        "\r\n",
        "1,112,3.5,1094785740\r",
        "\r\n",
        "1,151,4.0,1094785734\r",
        "\r\n",
        "1,223,4.0,1112485573\r",
        "\r\n",
        "1,253,4.0,1112484940\r",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head $moviesFilename"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "movieId,title,genres\r",
        "\r\n",
        "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r",
        "\r\n",
        "2,Jumanji (1995),Adventure|Children|Fantasy\r",
        "\r\n",
        "3,Grumpier Old Men (1995),Comedy|Romance\r",
        "\r\n",
        "4,Waiting to Exhale (1995),Comedy|Drama|Romance\r",
        "\r\n",
        "5,Father of the Bride Part II (1995),Comedy\r",
        "\r\n",
        "6,Heat (1995),Action|Crime|Thriller\r",
        "\r\n",
        "7,Sabrina (1995),Comedy|Romance\r",
        "\r\n",
        "8,Tom and Huck (1995),Adventure|Children\r",
        "\r\n",
        "9,Sudden Death (1995),Action\r",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we create two RDDs from the files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawRatings = sc.textFile(ratingsFilename)\n",
      "rawMovies = sc.textFile(moviesFilename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that up to this point no work has been done: the RDDs are created only when needed.\n",
      "\n",
      "Let's check out the ratings RDD."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawRatings.take(5)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[u'userId,movieId,rating,timestamp',\n",
        " u'1,2,3.5,1112486027',\n",
        " u'1,29,3.5,1112484676',\n",
        " u'1,32,3.5,1112484819',\n",
        " u'1,47,3.5,1112484727']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The RDDs consist on lines. We want to convert those to structured tuples to be able to use the Spark functions with the data. Let's do that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def removeHeader(rdd):\n",
      "    header = rdd.first() #extract header\n",
      "    rdd = rdd.filter(lambda x: x != header)    #filter out header\n",
      "    return rdd\n",
      "\n",
      "def parseRatings(line):\n",
      "    items = line.split(',')\n",
      "    # userid , movieid , rating , timestamp\n",
      "    return int(items[0]), int(items[1]), float(items[2])\n",
      "\n",
      "def parseMovies(line):\n",
      "    items = line.split(',')\n",
      "    # movieid,name,genre1|genre2...\n",
      "    return int(items[0]), items[1]   # drop the genres here\n",
      "\n",
      "ratings = removeHeader(rawRatings).map(parseRatings).cache()  # cache is a transformation, so it's lazy!\n",
      "movies = removeHeader(rawMovies).map(parseMovies).cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that we are caching the resulting RDDs to avoid having to read from disk again in future calculations.\n",
      "\n",
      "Now the RDDs are tuple-like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratings.take(3)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[(1, 2, 3.5), (1, 29, 3.5), (1, 32, 3.5)]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movies.take(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[(1, u'Toy Story (1995)'),\n",
        " (2, u'Jumanji (1995)'),\n",
        " (3, u'Grumpier Old Men (1995)')]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we ran the commands above again it will take MUCH shorter time because the RDDs are now cached."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analysis using the RDD interface"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are ready to start our analysis.\n",
      "\n",
      "First, how many ratings and movies do we have in the dataset?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'N Ratings: {}'.format(ratings.count())\n",
      "print 'N Movies: {}'.format(movies.count())"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N Ratings: 20000263\n",
        "N Movies: 27278"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We would like to know which ones are the top rated movies in average. To do that, we need to calculate the average rating by movieid. We do that calculating first the sum of ratings and the number of ratings for each movie, and then dividing these values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# (movieid, (sumratings, nratings))\n",
      "sumratings_nratings_by_movie = (ratings\n",
      "     .map(lambda rating: (rating[1], (rating[2], 1)))\n",
      "     .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
      "\n",
      "# (movieid, avgrating)\n",
      "avg_rating_by_movie = (sumratings_nratings_by_movie\n",
      "     .mapValues(lambda value: value[0] / float(value[1]))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see the top 10 rated movies:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_rating_by_movie.takeOrdered(10, lambda x: -x[1])"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[(106113, 5.0),\n",
        " (105841, 5.0),\n",
        " (94657, 5.0),\n",
        " (129905, 5.0),\n",
        " (94737, 5.0),\n",
        " (114193, 5.0),\n",
        " (126945, 5.0),\n",
        " (117314, 5.0),\n",
        " (102882, 5.0),\n",
        " (118610, 5.0)]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ugh, we get the ids and we would like to see the full titles. Luckily, RDDs can be joined, so we just (inner) join our average ratings RDD with the movies RDD.\n",
      "\n",
      "Joins are performed on pair RDDs by key, and the result is another RDD (surprise!) where the key is the shared key and the values are the values of the first and the second RDD, in order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movieid_rating_movie = avg_rating_by_movie.join(movies)\n",
      "movieid_rating_movie.takeOrdered(10, lambda x: -x[1][0])"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[(99450, (5.0, u'Sun Kissed (2012)')),\n",
        " (79866, (5.0, u'Schmatta: Rags to Riches to Rags (2009)')),\n",
        " (119430, (5.0, u'Yonkers Joe (2008)')),\n",
        " (111546, (5.0, u'Paying the Price: Killing the Children of Iraq (2000)')),\n",
        " (88488, (5.0, u'\"Summer Wishes')),\n",
        " (130644,\n",
        "  (5.0, u'The Garden of Sinners - Chapter 5: Paradox Paradigm (2008)')),\n",
        " (116227, (5.0, u'Taxi Blues (1990)')),\n",
        " (126397, (5.0, u'The Encounter (2010)')),\n",
        " (103753, (5.0, u'\"Human Behavior Experiments')),\n",
        " (95977, (5.0, u'Junior Prom (1946)'))]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uh, these are really the best movies? The ranking is biased because of some movies with very few ratings but all of them good. Let's solve this by taking only the movies with 500 ratings or more."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (movieid, (sumratings, nratings))\n",
      "movieid_rating_movie_popular = (sumratings_nratings_by_movie\n",
      "                                .filter(lambda x: x[1][1] >= 500)  # filter to get those with more than 500 ratings\n",
      "                                .mapValues(lambda value: value[0] / float(value[1]))  # compute average\n",
      "                                .join(movies)  # include names\n",
      ")\n",
      "movieid_rating_movie_popular.takeOrdered(10, lambda x: -x[1][0])"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[(318, (4.446990499637029, u'\"Shawshank Redemption')),\n",
        " (858, (4.364732196832306, u'\"Godfather')),\n",
        " (50, (4.334372207803259, u'\"Usual Suspects')),\n",
        " (527, (4.310175010988133, u\"Schindler's List (1993)\")),\n",
        " (1221, (4.275640557704942, u'\"Godfather: Part II')),\n",
        " (2019, (4.2741796572216, u'Seven Samurai (Shichinin no samurai) (1954)')),\n",
        " (904, (4.271333600779414, u'Rear Window (1954)')),\n",
        " (7502, (4.263182346109176, u'Band of Brothers (2001)')),\n",
        " (912, (4.258326830670664, u'Casablanca (1942)')),\n",
        " (922, (4.256934865900383, u'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)'))]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, now it makes a bit more sense. :D"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis using the DataFrame API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned before, the DataFrame API was introduced with Spark 1.3. It closely mimics the API of a Pandas dataframe, and in fact Spark DataFrames can be converted to Pandas dataframes and viceversa.\n",
      "\n",
      "Spark DataFrames have a rich API we can use to perform the previous computations in a much more human way.\n",
      "\n",
      "But first we have to convert our RDDs to DataFrames!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql import SQLContext, Row\n",
      "sqlContext = SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseRatingsDF(line):\n",
      "    parts = line.split(',')\n",
      "    # userid :: movieid :: rating :: timestamp\n",
      "    return Row(userid=int(parts[0]), movieid=int(parts[1]), rating=float(parts[2]))\n",
      "\n",
      "def parseMoviesDF(line):\n",
      "    parts = line.split(',')\n",
      "    # movieid::name::genre1|genre2...\n",
      "    return Row(movieid=int(parts[0]), name=parts[1])\n",
      "\n",
      "df_ratings = sqlContext.createDataFrame(removeHeader(rawRatings).map(parseRatingsDF))\n",
      "df_movies = sqlContext.createDataFrame(removeHeader(rawMovies).map(parseMoviesDF))\n",
      "\n",
      "df_ratings.cache()\n",
      "df_movies.cache()"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "DataFrame[movieid: bigint, name: string]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can get the average rating and # of ratings of each movie muuuuch more easily."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_rating_nratings_by_movie = df_ratings.groupBy('movieid').agg({'rating': 'mean', '*': 'count'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_rating_nratings_by_movie.sort('avg(rating)', ascending=False).take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[Row(movieid=94431, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=129034, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=107434, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=72235, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=129036, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=121039, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=122441, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=105841, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=129243, avg(rating)=5.0, count(1)=1),\n",
        " Row(movieid=99243, avg(rating)=5.0, count(1)=1)]"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we need to join the ratings dataframe with the movies one to get the titles, and exclude movies with less than 500 ratings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movies_name_avgrating = (avg_rating_nratings_by_movie.join(df_movies, on='movieid')\n",
      "                         .filter(avg_rating_nratings_by_movie['count(1)'] >= 500)\n",
      "                         .select('name', 'avg(rating)'))\n",
      "movies_name_avgrating.sort('avg(rating)', ascending=False).take(10)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "[Row(name=u'\"Shawshank Redemption', avg(rating)=4.446990499637029),\n",
        " Row(name=u'\"Godfather', avg(rating)=4.364732196832306),\n",
        " Row(name=u'\"Usual Suspects', avg(rating)=4.334372207803259),\n",
        " Row(name=u\"Schindler's List (1993)\", avg(rating)=4.310175010988133),\n",
        " Row(name=u'\"Godfather: Part II', avg(rating)=4.275640557704942),\n",
        " Row(name=u'Seven Samurai (Shichinin no samurai) (1954)', avg(rating)=4.2741796572216),\n",
        " Row(name=u'Rear Window (1954)', avg(rating)=4.271333600779414),\n",
        " Row(name=u'Band of Brothers (2001)', avg(rating)=4.263182346109176),\n",
        " Row(name=u'Casablanca (1942)', avg(rating)=4.258326830670664),\n",
        " Row(name=u'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)', avg(rating)=4.256934865900383)]"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One cool thing is that we can register DataFrames as \"tables\" and use SQL to query them. Miquel you will like this! ;)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_ratings.registerTempTable(\"ratings\")\n",
      "df_movies.registerTempTable(\"movies\")\n",
      "\n",
      "movie_avgrating = sqlContext.sql(\"\"\"\n",
      "SELECT ratings.movieid, AVG(rating) as avgrating, name, count(*) as nratings\n",
      "FROM ratings join movies on ratings.movieid = movies.movieid\n",
      "GROUP BY ratings.movieid, name \n",
      "HAVING nratings >= 500\n",
      "ORDER BY avgrating desc\n",
      "\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movie_avgrating.take(10)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[Row(movieid=318, avgrating=4.446990499637029, name=u'\"Shawshank Redemption', nratings=63366),\n",
        " Row(movieid=858, avgrating=4.364732196832306, name=u'\"Godfather', nratings=41355),\n",
        " Row(movieid=50, avgrating=4.334372207803259, name=u'\"Usual Suspects', nratings=47006),\n",
        " Row(movieid=527, avgrating=4.310175010988133, name=u\"Schindler's List (1993)\", nratings=50054),\n",
        " Row(movieid=1221, avgrating=4.275640557704942, name=u'\"Godfather: Part II', nratings=27398),\n",
        " Row(movieid=2019, avgrating=4.2741796572216, name=u'Seven Samurai (Shichinin no samurai) (1954)', nratings=11611),\n",
        " Row(movieid=904, avgrating=4.271333600779414, name=u'Rear Window (1954)', nratings=17449),\n",
        " Row(movieid=7502, avgrating=4.263182346109176, name=u'Band of Brothers (2001)', nratings=4305),\n",
        " Row(movieid=912, avgrating=4.258326830670664, name=u'Casablanca (1942)', nratings=24349),\n",
        " Row(movieid=922, avgrating=4.256934865900383, name=u'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)', nratings=6525)]"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Collaborative filtering by Alternating Least Squares\n",
      "\n",
      "\n",
      "With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n",
      "\n",
      "![factorization](http://spark-mooc.github.io/web-assets/images/matrix_factorization.png)\n",
      "\n",
      "We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The [Alternating Least Squares][als] algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constrant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n",
      "\n",
      "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
      "\n",
      "[als]: https://en.wikiversity.org/wiki/Least-Squares_Method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "training_ratings, validation_ratings, test_ratings = ratings.randomSplit([6, 2, 2], seed=0L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "from pyspark.mllib.evaluation import RegressionMetrics\n",
      "\n",
      "# iduser, idmovie, rating\n",
      "reformat = lambda x: ((x[0], x[1]), x[2])\n",
      "\n",
      "validation_iduser_idmovie = validation_ratings.map(lambda x: (x[0], x[1]))\n",
      "validation_rating_by_iduser_idmovie = validation_ratings.map(reformat)\n",
      "\n",
      "seed = 5L\n",
      "iterations = 2\n",
      "regularizationParameter = 0.1\n",
      "ranks = [4, 8, 12]  # matrix ranks\n",
      "errors = [0, 0, 0]\n",
      "err_ix = 0\n",
      "tolerance = 0.03\n",
      "\n",
      "minError = float('inf')\n",
      "bestRank = -1\n",
      "bestIteration = -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for rank in ranks:\n",
      "    model = ALS.train(training_ratings, rank, seed=seed, iterations=iterations,\n",
      "                      lambda_=regularizationParameter)\n",
      "    validation_userid_movieid_predictedrating = model.predictAll(validation_iduser_idmovie)\n",
      "    validation_predictedrating_by_iduser_idmovie = (validation_userid_movieid_predictedrating\n",
      "                                                    .map(reformat))\n",
      "    validation_prediction_observations = (validation_predictedrating_by_iduser_idmovie\n",
      "                                          .join(validation_rating_by_iduser_idmovie)\n",
      "                                          .map(lambda x: (x[1][0], x[1][1])))\n",
      "    error = RegressionMetrics(validation_prediction_observations).meanSquaredError\n",
      "    errors[err_ix] = error\n",
      "    err_ix += 1\n",
      "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
      "    if error < minError:\n",
      "        minError = error\n",
      "        bestRank = rank\n",
      "\n",
      "print 'The best model was trained with rank %s' % bestRank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For rank 4 the RMSE is 0.819083246867\n",
        "For rank 8 the RMSE is 0.793921268663\n",
        "For rank 12 the RMSE is 0.787600047784\n",
        "The best model was trained with rank 12\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}